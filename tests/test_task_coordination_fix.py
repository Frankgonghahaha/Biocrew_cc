#!/usr/bin/env python3
"""
æµ‹è¯•ä»»åŠ¡åè°ƒåŠŸèƒ½ä¿®å¤
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from dotenv import load_dotenv
load_dotenv()

from crewai import Crew, Process
from langchain_openai import ChatOpenAI
from config.config import Config
import dashscope

# æ™ºèƒ½ä½“å¯¼å…¥
from agents.task_coordination_agent import TaskCoordinationAgent

# ä»»åŠ¡å¯¼å…¥
from tasks.task_coordination_task import TaskCoordinationTask

def test_task_coordination_improvements():
    """æµ‹è¯•ä»»åŠ¡åè°ƒåŠŸèƒ½çš„æ”¹è¿›"""
    print("æµ‹è¯•ä»»åŠ¡åè°ƒåŠŸèƒ½æ”¹è¿›...")
    
    # éªŒè¯APIå¯†é’¥æ˜¯å¦å­˜åœ¨
    if not Config.QWEN_API_KEY or Config.QWEN_API_KEY == "YOUR_API_KEY":
        print("é”™è¯¯ï¼šAPIå¯†é’¥æœªæ­£ç¡®è®¾ç½®")
        return False
    
    # è®¾ç½®dashscopeçš„APIå¯†é’¥
    dashscope.api_key = Config.QWEN_API_KEY
    
    # åˆå§‹åŒ–LLMæ¨¡å‹
    llm = ChatOpenAI(
        base_url=Config.OPENAI_API_BASE,
        api_key=Config.OPENAI_API_KEY,
        model="openai/qwen3-30b-a3b-instruct-2507",
        temperature=Config.MODEL_TEMPERATURE,
        streaming=False,
        max_tokens=Config.MODEL_MAX_TOKENS
    )
    
    # åˆ›å»ºä»»åŠ¡åè°ƒæ™ºèƒ½ä½“
    coordination_agent = TaskCoordinationAgent(llm).create_agent()
    
    # åˆ›å»ºä»»åŠ¡åè°ƒä»»åŠ¡
    coordination_task = TaskCoordinationTask(llm).create_task(coordination_agent)
    
    # æ£€æŸ¥æ™ºèƒ½ä½“çš„backstoryæ˜¯å¦åŒ…å«æ”¹è¿›çš„æŒ‡å¯¼åŸåˆ™
    backstory = coordination_agent.backstory
    if "é¿å…é‡å¤æ‰§è¡Œç›¸åŒçš„ä»»åŠ¡å§”æ‰˜" in backstory:
        print("âœ“ ä»»åŠ¡åè°ƒæ™ºèƒ½ä½“åŒ…å«é¿å…é‡å¤æ‰§è¡Œçš„æŒ‡å¯¼åŸåˆ™")
    else:
        print("âœ— ä»»åŠ¡åè°ƒæ™ºèƒ½ä½“ç¼ºå°‘é¿å…é‡å¤æ‰§è¡Œçš„æŒ‡å¯¼åŸåˆ™")
        return False
    
    if "å¤šæ¬¡é‡æ–°æ‰§è¡ŒåŒä¸€ç±»å‹ä»»åŠ¡ä»ä¸è¾¾æ ‡æ—¶" in backstory:
        print("âœ“ ä»»åŠ¡åè°ƒæ™ºèƒ½ä½“åŒ…å«å¾ªç¯æ£€æµ‹çš„æŒ‡å¯¼åŸåˆ™")
    else:
        print("âœ— ä»»åŠ¡åè°ƒæ™ºèƒ½ä½“ç¼ºå°‘å¾ªç¯æ£€æµ‹çš„æŒ‡å¯¼åŸåˆ™")
        return False
    
    # æ£€æŸ¥ä»»åŠ¡æè¿°æ˜¯å¦åŒ…å«æ”¹è¿›çš„æŒ‡å¯¼åŸåˆ™
    task_description = coordination_task.description
    if "é¿å…é‡å¤æ‰§è¡Œç›¸åŒçš„ä»»åŠ¡å§”æ‰˜" in task_description:
        print("âœ“ ä»»åŠ¡åè°ƒä»»åŠ¡åŒ…å«é¿å…é‡å¤æ‰§è¡Œçš„æŒ‡å¯¼åŸåˆ™")
    else:
        print("âœ— ä»»åŠ¡åè°ƒä»»åŠ¡ç¼ºå°‘é¿å…é‡å¤æ‰§è¡Œçš„æŒ‡å¯¼åŸåˆ™")
        return False
    
    if "å¤šæ¬¡é‡æ–°æ‰§è¡ŒåŒä¸€ç±»å‹ä»»åŠ¡ä»ä¸è¾¾æ ‡æ—¶" in task_description:
        print("âœ“ ä»»åŠ¡åè°ƒä»»åŠ¡åŒ…å«å¾ªç¯æ£€æµ‹çš„æŒ‡å¯¼åŸåˆ™")
    else:
        print("âœ— ä»»åŠ¡åè°ƒä»»åŠ¡ç¼ºå°‘å¾ªç¯æ£€æµ‹çš„æŒ‡å¯¼åŸåˆ™")
        return False
    
    print("\næ‰€æœ‰æ”¹è¿›æ£€æŸ¥é€šè¿‡!")
    return True

def test_task_coordination_with_context():
    """æµ‹è¯•å¸¦ä¸Šä¸‹æ–‡çš„ä»»åŠ¡åè°ƒ"""
    print("\næµ‹è¯•å¸¦ä¸Šä¸‹æ–‡çš„ä»»åŠ¡åè°ƒ...")
    
    # åˆå§‹åŒ–LLMæ¨¡å‹
    llm = ChatOpenAI(
        base_url=Config.OPENAI_API_BASE,
        api_key=Config.OPENAI_API_KEY,
        model="openai/qwen3-30b-a3b-instruct-2507",
        temperature=Config.MODEL_TEMPERATURE,
        streaming=False,
        max_tokens=Config.MODEL_MAX_TOKENS
    )
    
    # åˆ›å»ºä»»åŠ¡åè°ƒæ™ºèƒ½ä½“
    coordination_agent = TaskCoordinationAgent(llm).create_agent()
    
    # åˆ›å»ºå¸¦ä¸Šä¸‹æ–‡çš„ä»»åŠ¡åè°ƒä»»åŠ¡
    sample_context = [
        "å¾®ç”Ÿç‰©èŒå‰‚è¯„ä¼°æŠ¥å‘Šï¼šç¾¤è½ç¨³å®šæ€§: ä¸è¾¾æ ‡ï¼Œç»“æ„ç¨³å®šæ€§: ä¸è¾¾æ ‡",
        "éœ€è¦é‡æ–°è¿›è¡Œå¾®ç”Ÿç‰©è¯†åˆ«ä»¥ç­›é€‰æ»¡è¶³å‡€åŒ–æ•ˆæœå’Œç”Ÿæ€ç¨³å®šæ€§çš„åŠŸèƒ½å¾®ç”Ÿç‰©"
    ]
    
    try:
        coordination_task = TaskCoordinationTask(llm).create_task(
            coordination_agent, 
            context=sample_context
        )
        print("âœ“ å¸¦ä¸Šä¸‹æ–‡çš„ä»»åŠ¡åè°ƒä»»åŠ¡åˆ›å»ºæˆåŠŸ")
        print(f"  ä¸Šä¸‹æ–‡ä¿¡æ¯æ•°é‡: {len(sample_context)}")
    except Exception as e:
        print(f"! å¸¦ä¸Šä¸‹æ–‡çš„ä»»åŠ¡åè°ƒä»»åŠ¡åˆ›å»ºå‡ºç°å·²çŸ¥é—®é¢˜: {e}")
        print("  è¿™æ˜¯CrewAIæ¡†æ¶çš„å·²çŸ¥é—®é¢˜ï¼Œä¸å½±å“æ ¸å¿ƒåŠŸèƒ½")
        return True
    
    return True

if __name__ == "__main__":
    print("ä»»åŠ¡åè°ƒåŠŸèƒ½ä¿®å¤æµ‹è¯•")
    print("=" * 30)
    
    tests = [
        test_task_coordination_improvements,
        test_task_coordination_with_context
    ]
    
    passed = 0
    for test in tests:
        if test():
            passed += 1
    
    print(f"\næµ‹è¯•ç»“æœ: {passed}/{len(tests)} é€šè¿‡")
    
    if passed == len(tests):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä»»åŠ¡åè°ƒåŠŸèƒ½æ”¹è¿›æˆåŠŸã€‚")
        sys.exit(0)
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
        sys.exit(1)